{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn-LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码示例：https://mp.weixin.qq.com/s/hMcJtB3Lss1NBalXRTGZlQ （玉树芝兰） <br>\n",
    "可视化：https://blog.csdn.net/qq_39496504/article/details/107125284  <br>\n",
    "sklearn lda参数解读:https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "<br>中文版参数解读：https://blog.csdn.net/TiffanyRabbit/article/details/76445909\n",
    "<br>LDA原理-视频版：https://www.bilibili.com/video/BV1t54y127U8\n",
    "<br>LDA原理-文字版：https://www.jianshu.com/p/5c510694c07e\n",
    "<br>score的计算方法：https://github.com/scikit-learn/scikit-learn/blob/844b4be24d20fc42cc13b957374c718956a0db39/sklearn/decomposition/_lda.py#L729\n",
    "<br>主题困惑度1：https://blog.csdn.net/weixin_43343486/article/details/109255165\n",
    "<br>主题困惑度2：https://blog.csdn.net/weixin_39676021/article/details/112187210"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg as psg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: 'D:/python/lda/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GENGCH~1\\AppData\\Local\\Temp/ipykernel_2588/2830034216.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:/python/lda/result'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:/python/lda/data'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#content type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: 'D:/python/lda/data'"
     ]
    }
   ],
   "source": [
    "output_path = './result'\n",
    "file_path = './data'\n",
    "os.chdir(file_path)\n",
    "data=pd.read_excel(\"./新建文本文档.txt\")#content type\n",
    "os.chdir(output_path)\n",
    "dic_file = \"./dict.txt\"\n",
    "stop_file = \"./stopwordslist.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_word_cut(mytext):\n",
    "    jieba.load_userdict(dic_file)\n",
    "    jieba.initialize()\n",
    "    try:\n",
    "        stopword_list = open(stop_file,encoding ='utf-8')\n",
    "    except:\n",
    "        stopword_list = []\n",
    "        print(\"error in stop_file\")\n",
    "    stop_list = []\n",
    "    flag_list = ['n','nz','vn']\n",
    "    for line in stopword_list:\n",
    "        line = re.sub(u'\\n|\\\\r', '', line)\n",
    "        stop_list.append(line)\n",
    "    \n",
    "    word_list = []\n",
    "    #jieba分词\n",
    "    seg_list = psg.cut(mytext)\n",
    "    for seg_word in seg_list:\n",
    "        word = re.sub(u'[^\\u4e00-\\u9fa5]','',seg_word.word)\n",
    "        #word = seg_word.word  #如果想要分析英语文本，注释这行代码，启动下行代码\n",
    "        find = 0\n",
    "        for stop_word in stop_list:\n",
    "            if stop_word == word or len(word)<2:     #this word is stopword\n",
    "                    find = 1\n",
    "                    break\n",
    "        if find == 0 and seg_word.flag in flag_list:\n",
    "            word_list.append(word)      \n",
    "    return (\" \").join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"content_cutted\"] = data.content.apply(chinese_word_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.LDA分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    tword = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        topic_w = \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        tword.append(topic_w)\n",
    "        print(topic_w)\n",
    "    return tword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000 #提取1000个特征词语\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(data.content_cutted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 8\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=50,\n",
    "                                learning_method='batch',\n",
    "                                learning_offset=50,\n",
    "#                                 doc_topic_prior=0.1,\n",
    "#                                 topic_word_prior=0.01,\n",
    "                               random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1输出每个主题对应词语 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 25\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "topic_word = print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2输出每篇文章对应主题 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics=lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = []\n",
    "for t in topics:\n",
    "    topic.append(\"Topic #\"+str(list(t).index(np.max(t))))\n",
    "data['概率最大的主题序号']=topic\n",
    "data['每个主题对应概率']=list(topics)\n",
    "data.to_excel(\"data_topic.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3可视化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pic = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "pyLDAvis.display(pic)\n",
    "pyLDAvis.save_html(pic, 'lda_pass'+str(n_topics)+'.html')\n",
    "pyLDAvis.display(pic)\n",
    "#去工作路径下找保存好的html文件\n",
    "#和视频里讲的不一样，目前这个代码不需要手动中断运行，可以快速出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4困惑度 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plexs = []\n",
    "scores = []\n",
    "n_max_topics = 16\n",
    "for i in range(1,n_max_topics):\n",
    "    print(i)\n",
    "    lda = LatentDirichletAllocation(n_components=i, max_iter=50,\n",
    "                                    learning_method='batch',\n",
    "                                    learning_offset=50,random_state=0)\n",
    "    lda.fit(tf)\n",
    "    plexs.append(lda.perplexity(tf))\n",
    "    scores.append(lda.score(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t=15#区间最右侧的值。注意：不能大于n_max_topics\n",
    "x=list(range(1,n_t+1))\n",
    "plt.plot(x,plexs[0:n_t])\n",
    "plt.xlabel(\"number of topics\")\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "303.324px",
    "left": "114px",
    "top": "110.322px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
